{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project: Flights delays prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the task is:\n",
    "\n",
    "> **Problem statement**: If we are given information on US commercial flights in January 2020, check if a flight is delayed, where delay means that the flight arrives at its destination at least 15 minutes after arrival time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "def describe(a):\n",
    "    if type(a) is np.ndarray:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\ntype: {}\".format(a, a.shape, a.dtype, type(a)))\n",
    "    elif type(a) is pd.Series:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ndtype:{}\\nname:{}\\nindex-name:{}\\nindex-type:{}\\ntype:{}\".format(a, a.shape, a.dtype, a.name, a.index.name,type(a.index), type(a)))\n",
    "    elif type(a) is pd.DataFrame:\n",
    "        print(\"data:\\n{}\\nshape:{}\\ntype:{}\".format(a, a.shape,type(a)))\n",
    "    else:\n",
    "        print(\"{}, type:{}\".format(a, type(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets loading\n",
    "\n",
    "- **Jan_2020_ontime**\n",
    "- **airports_data**\n",
    "- **airlines_data**\n",
    "\n",
    "**Jan_2020_ontime** is composed by 607346 rows and 21 columns,\n",
    "\n",
    "**airports_data** and **airlines_data** are respective composed by: 342 rows and 7 columns and 20 rows and 2 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"dataset/Jan_2020_ontime.csv\")\n",
    "airports_data = pd.read_csv(\"dataset/airports.csv\")\n",
    "airlines_data = pd.read_csv(\"dataset/airlines.csv\")\n",
    "\n",
    "dataset_1.info()\n",
    "airports_data.info()\n",
    "airlines_data.info()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally **airport_data** was characterized by some null values. Specifically, only 4 rows has null values at **CITY** and **STATE**.\n",
    "\n",
    "![](images/update%20airports.png).\n",
    "\n",
    "The problem was resolved by update manually the missing informations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data = dataset_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first exploration of **delays_data** revealed that the last column is useless, so it will be delete. Furthermore exists 36 airport contained in **delays_data** but not in **airports_data**, below iata code of missing airports.\n",
    "\n",
    "    'ECP', 'SWO', 'HVN', 'HHH', 'STS', 'PBG', 'IAG', 'LBE', 'XWA', 'SLN', 'CKB', 'OGS', 'LBL', 'EAR', 'UIN', 'LBF', 'BFF', 'CGI', 'ATY', 'SCK', 'PRC', 'PAE', 'PGD', 'AZA', 'PVU', 'SFB', 'USA', 'BLV', 'LCK', 'HGR', 'BFM', 'PSM', 'OWB', 'OGD', 'SHR', 'RIW'.\n",
    "\n",
    "\n",
    "Due to the impossibility of plotting these airports on a graph becouse of the missing data such as LATITUDE and LONGITUDE, it has been decided to delete these airports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(delays_data):\n",
    "    origin = Series(delays_data[\"ORIGIN\"].unique())\n",
    "    destination = Series(delays_data[\"DEST\"].unique())\n",
    "    airports_delays = pd.concat([origin,destination]).drop_duplicates().reset_index(drop=True).values\n",
    "    airports_data_series = Series(np.arange(airports_data[\"IATA\"].values.size), index = airports_data[\"IATA\"])\n",
    "    missing_airports = []\n",
    "    \n",
    "    i = 0\n",
    "    for airport in airports_delays:\n",
    "        if(not(airport in airports_data_series)): \n",
    "            i+=1\n",
    "            missing_airports.append(airport)\n",
    "    print(\"Total of missing airports:\",i)\n",
    "    print(\"Details:\\n\", missing_airports)\n",
    "\n",
    "    origin_series = Series(np.arange(delays_data.shape[0]), index = delays_data[\"ORIGIN\"])\n",
    "    dest_series = Series(np.arange(delays_data.shape[0]), index = delays_data[\"DEST\"])\n",
    "\n",
    "    rows = []\n",
    "    for airport in missing_airports:        \n",
    "        rows.append(origin_series[airport].values)\n",
    "        rows.append(dest_series[airport].values)\n",
    "    \n",
    "    rows_to_be_deleted = []\n",
    "    for l in rows:\n",
    "        for e in l:\n",
    "            rows_to_be_deleted.append(e)\n",
    "\n",
    "    delays_data.drop(rows_to_be_deleted,inplace = True)\n",
    "\n",
    "    delays_data.reset_index(inplace=True)\n",
    "    delays_data.drop(columns = \"index\",inplace=True)\n",
    "\n",
    "    origin = Series(delays_data[\"ORIGIN\"].unique())\n",
    "    destination = Series(delays_data[\"DEST\"].unique())\n",
    "    airports_delays = pd.concat([origin,destination]).drop_duplicates().reset_index(drop=True).values\n",
    "\n",
    "    missing_airports = []\n",
    "    i = 0\n",
    "    for airport in airports_delays:\n",
    "        if(not(airport in airports_data_series)): \n",
    "            i+=1\n",
    "            missing_airports.append(airport)\n",
    "    print(\"Total of missing airports:\",i)\n",
    "    print(\"Details:\\n\", missing_airports)\n",
    "    delays_data.info()\n",
    "    print(\"-------------end data_cleaning\\n\")\n",
    "    \n",
    "#it prove that missing airports no longer contained in cleaned data\n",
    "data_cleaning(delays_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "In order to decrease the dimension of dataset, it has been decided to delete the following columns:\n",
    "\n",
    "    1. DEST_AIRPORT_ID\n",
    "    2. ORIGIN_AIRPORT_ID\n",
    "    3. OP_CARRIER_AIRLINE_ID\n",
    "    4. OP_UNIQUE_CARRIER\n",
    "    5. ORIGIN_AIRPORT_SEQ_ID\n",
    "    6. DEST_AIRPORT_SEQ_ID\n",
    "\n",
    "becouse of they're coding redundat information.\n",
    " \n",
    "More specifically:\n",
    "1. **OP_UNIQUE_CARRIER** e **OP_CARRIER_AIRLINE_ID**, are repetition of **OP_CARRIER**,\n",
    "2. **ORIGIN_AIRPORT_SEQ_ID** e **ORIGIN_AIRPORT_ID** are repetition of **ORIGIN**,\n",
    "3. **ORIGIN_AIRPORT_SEQ_ID** e **DEST_AIRPORT_ID** are repetition of **DEST**\n",
    "\n",
    "Furthermore, as mentioned before, it has been deleted **Unnamed: 21** columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_attributes(delays_data):\n",
    "    print(\"before\")\n",
    "    delays_data.info()\n",
    "    delays_data.drop(columns=[\"DEST_AIRPORT_ID\",\"ORIGIN_AIRPORT_ID\",\"OP_CARRIER_AIRLINE_ID\",\"OP_UNIQUE_CARRIER\",\"ORIGIN_AIRPORT_SEQ_ID\",\"DEST_AIRPORT_SEQ_ID\",\"Unnamed: 21\"],inplace=True)\n",
    "    print(\"after\")\n",
    "    delays_data.info()\n",
    "    print(\"------------end remove_useless_attributs \\n\")\n",
    "remove_useless_attributes(delays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data[delays_data.DIVERTED==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, only 1106 flights on 598841 are affected by **DIVERTED** status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values\n",
    "\n",
    "These attibutes are characterized by null values:\n",
    "\n",
    "    1. TAIL_NUM    (type object)\n",
    "    2. DEP_TIME    (type float64)\n",
    "    3. DEP_DEL15   (type float64)\n",
    "    4. ARR_TIME    (type float64)\n",
    "    5. ARR_DEL15   (type float64)\n",
    "\n",
    "It has been decided to delete all records that are characterized at least by a column containing null value. This choise has been made, taking into consideration that all the automatic generation algorithm for null values (e.x. replace by mean) cannot be applied to this particular problem. Moreover, the largeness of the dataset allow the deletions of rows without loss of generality. \n",
    "\n",
    "Finally, it should be noted that the elimination of all rows characterized by null values ​​in correspondence of the **ARR_TIME** and **ARR_DEL15** columns entails the total loss of the information about the cancellation of the flight as all the canceled flights are characterized by null values ​​in correspondence of the aforementioned columns. For this reason the **CANCELED** feature is deleted. This elimination does not involve any problem as the information about the canceled flights has no weight on the flight delay, as canceled flights have never taken off, therefore the delay is not definable for these particular records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(delays_data):\n",
    "    print(\"before\")\n",
    "    delays_data.info()\n",
    "    delays_data.dropna(subset = [\"TAIL_NUM\",\"DEP_TIME\",\"DEP_DEL15\",\"ARR_TIME\",\"ARR_DEL15\"],inplace=True)\n",
    "    delays_data.reset_index(inplace=True)\n",
    "    delays_data.drop(columns = \"index\",inplace=True)    \n",
    "    print(\"after\")\n",
    "    delays_data.info()\n",
    "    print(\"-----------end remove_null\\n\")    \n",
    "remove_null(delays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data[delays_data.DIVERTED==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regards the **DIVERTED** attribute (diverted flight), the elimination of rows containing null values ​​on the columns defined above brings to zero the number of flights affected by this status.\n",
    "\n",
    "For this reason, in addition to the **CANCELED** attribute, **DIVERTED** is also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.drop(columns=[\"CANCELLED\",\"DIVERTED\"],inplace=True)\n",
    "delays_data.reset_index(inplace=True)\n",
    "delays_data.drop(columns = \"index\",inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define block structure for arrival time, such as **DEP_TIME_BLK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_time(x):\n",
    "  if x >= 600 and x <= 659:\n",
    "    return '0600-0659'\n",
    "  elif x>=1400 and x<=1459:\n",
    "    return '1400-1459'\n",
    "  elif x>=1200 and x<=1259:\n",
    "    return '1200-1259'\n",
    "  elif x>=1500 and x<=1559:\n",
    "    return '1500-1559'\n",
    "  elif x>=1900 and x<=1959:\n",
    "    return '1900-1959'\n",
    "  elif x>=900 and x<=959:\n",
    "    return '0900-0959'\n",
    "  elif x>=1000 and x<=1059:\n",
    "    return  '1000-1059'\n",
    "  elif x>=2000 and x<=2059:\n",
    "    return '2000-2059'\n",
    "  elif x>=1300 and x<=1359:\n",
    "    return '1300-1359'\n",
    "  elif x>=1100 and x<=1159:\n",
    "    return '1100-1159'\n",
    "  elif x>=800 and x<=859:\n",
    "    return '0800-0859'\n",
    "  elif x>=2200 and x<=2259:\n",
    "    return '2200-2259'\n",
    "  elif x>=1600 and x<=1659:\n",
    "    return '1600-1659'\n",
    "  elif x>=1700 and x<=1759:\n",
    "    return '1700-1759'\n",
    "  elif x>=2100 and x<=2159:\n",
    "    return '2100-2159'\n",
    "  elif x>=700 and x<=759:\n",
    "    return '0700-0759'\n",
    "  elif x>=1800 and x<=1859:\n",
    "    return '1800-1859'\n",
    "  elif x>=1 and x<=559:\n",
    "    return '0001-0559'\n",
    "  elif x>=2300 and x<=2400:\n",
    "    return '2300-2400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data['ARR_TIME_BLK'] = delays_data[\"ARR_TIME\"].astype(int).apply(lambda x :arr_time(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploratory analysis shows that exists numerical values that aren't conductible to times, such as mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data['DEP_TIME'] = delays_data['DEP_TIME'].astype(str)\n",
    "array_dep = delays_data['DEP_TIME'].to_numpy()\n",
    "\n",
    "for s in array_dep:\n",
    "    if(len(s)==2):\n",
    "        print(s)\n",
    "        break\n",
    "    if(len(s)==3):\n",
    "        print(s)\n",
    "        break\n",
    "    if(len(s)==4):\n",
    "        print(s)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_times(delays_data):\n",
    "    delays_data['DEP_TIME'] = delays_data['DEP_TIME'].astype(str)\n",
    "    delays_data['ARR_TIME'] = delays_data['ARR_TIME'].astype(str)\n",
    "\n",
    "    array_dep = delays_data['DEP_TIME'].to_numpy()\n",
    "    array_arr = delays_data['ARR_TIME'].to_numpy()\n",
    "\n",
    "    i = 0\n",
    "    ret = set()\n",
    "    for s in array_dep:\n",
    "        if(len(s)==2):\n",
    "            ret.add(i)\n",
    "        if(len(s)==3):\n",
    "            ret.add(i)\n",
    "        if(len(s)==4):\n",
    "            ret.add(i)\n",
    "        i+=1\n",
    "    i = 0\n",
    "    for s in array_arr:\n",
    "        if(len(s)==2):\n",
    "            ret.add(i)\n",
    "        if(len(s)==3):\n",
    "            ret.add(i)\n",
    "        if(len(s)==4):\n",
    "            ret.add(i)\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    delays_data.drop(ret,inplace=True)\n",
    "    delays_data.reset_index(inplace=True)\n",
    "    delays_data.drop(columns='index',inplace=True)\n",
    "remove_useless_times(delays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = delays_data['DEP_TIME'].to_numpy()\n",
    "array2 = delays_data['ARR_TIME'].to_numpy()\n",
    "\n",
    "i = 0\n",
    "for s in array:\n",
    "    if(len(s)==3):\n",
    "        i+=1\n",
    "        #print(s)    \n",
    "    if(len(s)==4):\n",
    "        i+=1\n",
    "        #print(s)\n",
    "\n",
    "for s in array2:\n",
    "    if(len(s)==3):\n",
    "        i+=1\n",
    "        #print(s)    \n",
    "    if(len(s)==4):\n",
    "        i+=1\n",
    "        #print(s) \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_preprocessing(time):\n",
    "    if(len(time)==5):\n",
    "        time = '0'+time[:1]+':'+time[1:3]\n",
    "        return time\n",
    "    else:\n",
    "        time = time[:2]+':'+time[2:4]\n",
    "        if(time[:2]=='24'):\n",
    "            time = '00'+time[2:]\n",
    "    return time\n",
    "\n",
    "def refactor(time):\n",
    "    time = time[:4]+'0'+time[5:]\n",
    "    return time\n",
    "\n",
    "from datetime import datetime\n",
    "def convert(time):\n",
    "    return datetime.strptime(time,\"%H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with a pre-processing of the **ARR_TIME** and **DEP_TIME** columns, in order to admit to these features the shape: \n",
    "    **%H:%M**.\n",
    "\n",
    "Subsequently, in order to generate a new function representing the flight duration, the values ​​relating to the aforementioned columns are converted into the datetime format in order to calculate the difference between the departure time and the arrival time.\n",
    "The new attribute (**DUR**) is therefore a pre-processing phase that involves the elimination of some non-useful data such as the days elapsed between arrival and departure. This information is deleted as all flights certainly last less than 24 hours.\n",
    "Note that, in order to avoid generating specific and not very reusable durations, it was decided to eliminate the information about the fraction of minutes. For example, a flight with duration **01: 15** will have a new duration equal to **01: 10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data['ARR_TIME'] = delays_data[\"ARR_TIME\"].astype(str).apply(lambda x : time_preprocessing(x))\n",
    "delays_data['DEP_TIME'] = delays_data[\"DEP_TIME\"].astype(str).apply(lambda x : time_preprocessing(x))\n",
    "\n",
    "delays_data['ARR_TIME'] = delays_data['ARR_TIME'].apply(lambda x : convert(x))\n",
    "delays_data['DEP_TIME'] = delays_data['DEP_TIME'].apply(lambda x : convert(x))\n",
    "\n",
    "delays_data[\"DUR\"] = (delays_data[\"ARR_TIME\"]-delays_data[\"DEP_TIME\"])\n",
    "delays_data['DUR'] = delays_data['DUR'].astype(str).map(lambda x : x[7:])\n",
    "delays_data['DUR'] = delays_data['DUR'].apply(lambda x : refactor(x))\n",
    "\n",
    "dur = delays_data['DUR'].to_numpy()\n",
    "i = 0\n",
    "ret = []\n",
    "for s in dur:\n",
    "    if('+' in s):\n",
    "        ret.append(i)\n",
    "    i+=1\n",
    "\n",
    "delays_data.drop(ret,inplace=True)\n",
    "delays_data.reset_index(inplace=True)\n",
    "delays_data.drop(columns='index',inplace=True)\n",
    "delays_data['DUR'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by the output (**65**), the new attribute features only 65 unique values ​​out of a total of more than 500,000 rows.\n",
    "Note that the choice to minimize the number of distinct values ​​is due to the application of the **OneHotEncoder** in the **Train Generation and Test set** section, as this encoding technique generates a new feature for each distinct value present in each of the columns characterizing the dataset, therefore it could expose the problem to the phenomenon of the curse of dimensionality if the number of features generated as a result of encoding were high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A step prior to generating Train and Test sets is the process of identifying core attributes.\n",
    "\n",
    "To perform this analysis, the dataset is transformed by a label encoder, so using a random forest classifier we will get some information that could guide us on how to choose the main attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data[delays_data.DEP_DEL15!=delays_data.ARR_DEL15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Encoder_df = LabelEncoder() \n",
    "feature_importance_df = delays_data.copy()\n",
    "\n",
    "feature_importance_df = feature_importance_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "feature_importance_df_x = feature_importance_df.drop('ARR_DEL15', axis = 1)\n",
    "feature_importance_df_y = feature_importance_df['ARR_DEL15']\n",
    "\n",
    "rnd_reg = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=40)\n",
    "rnd_reg.fit(feature_importance_df_x, feature_importance_df_y)\n",
    "attributes = feature_importance_df_x.columns\n",
    "importances = rnd_reg.feature_importances_\n",
    "index = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"Attribute importance\")\n",
    "p = plt.barh(range(len(index)), importances[index], color='r', align='center')\n",
    "plt.yticks(range(len(index)), attributes[index])\n",
    "plt.xlabel(\"Relative importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fraction of records that have different values ​​at the **DEP_DEL15** and **ARR_DEL15** attributes is 0.06%, which means that the class attribute (**ARR_DEL15**) is strongly correlated to the **DEP_DEL15** attribute. In order to avoid training the model incorrectly, giving excessive weight to the **DEP_DEL15** attribute, it was decided to delete this column as following the cleaning of the data every flight with delayed departure reaches its destination late.\n",
    "\n",
    "It would be incorrect to train the model by taking this information strongly into consideration as the delay is affected by a multiplicity of variables such as meteorological conditions that are not taken into consideration in solving this specific problem.\n",
    "\n",
    "The feature importance step is repeated in order to verify the stability of the results previously obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.drop(columns = \"DEP_DEL15\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Encoder_df = LabelEncoder() \n",
    "feature_importance_df = delays_data.copy()\n",
    "\n",
    "feature_importance_df = feature_importance_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "feature_importance_df_x = feature_importance_df.drop('ARR_DEL15', axis = 1)\n",
    "feature_importance_df_y = feature_importance_df['ARR_DEL15']\n",
    "\n",
    "rnd_reg = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=40)\n",
    "rnd_reg.fit(feature_importance_df_x, feature_importance_df_y)\n",
    "attributes = feature_importance_df_x.columns\n",
    "importances = rnd_reg.feature_importances_\n",
    "index = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title(\"Attribute importance\")\n",
    "p = plt.barh(range(len(index)), importances[index], color='r', align='center')\n",
    "plt.yticks(range(len(index)), attributes[index])\n",
    "plt.xlabel(\"Relative importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the relations of the attributes is of fundamental importance to select a set of attributes representing the maximum possible information. To do this, statistical measures such as correlation are used.\n",
    "\n",
    "Correlation is a measure that expresses the relationship between two variables and is widely used to describe simple relationships since it does not depend on a cause-effect relationship but on the tendency of one variable to change as a function of another.\n",
    "\n",
    "Note how the correlation measure depends on the type of features that characterize the dataset, in other words categorical attributes will employ correlation measures different from those expected for numeric attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cramer's V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cramer's V is a measure of the strength of the association between two categorical variables.\n",
    "Its value ranges from 0 to 1 where:\n",
    "\n",
    "    - 0 indicates no association between the two variables\n",
    "    - 1 indicates a perfect association between the two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "def corr_cramers_matrix(df_cat):\n",
    "    ret = pd.DataFrame(index=df_cat.columns, dtype='float64')\n",
    "    for col1 in df_cat.columns:\n",
    "        series = pd.Series(index=df_cat.columns, dtype='float64')\n",
    "        for col2 in df_cat.columns:\n",
    "            confusion_matrix = pd.crosstab(df_cat[col1], df_cat[col2])\n",
    "            cramer = cramers_v(confusion_matrix.values)\n",
    "            series.loc[col2] = cramer\n",
    "        ret[col1] = series\n",
    "    return ret\n",
    "\n",
    "corr_cramer = corr_cramers_matrix(delays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(15,12))\n",
    "    ax = sns.heatmap(corr_cramer,vmax=1,cmap=\"Blues\",square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis of the correlation between the attributes, it emerges that there is a significant correlation between the following pairs:\n",
    "\n",
    "    1. DAY_OF_MONTH and DAY_OF_WEEK,\n",
    "    2. OP_CARRIER and TAIL_NUMBER,\n",
    "    3. between the departure time and the relative block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute deletion\n",
    "\n",
    "Due to the strong correlation between the **OP_CARRIER** and **TAIL_NUM** attribute, it was decided to delete **TAIL_NUM**.\n",
    "\n",
    "The **OP_CARRIER_FL_NUM**, **ARR_TIME** and **DEP_TIME** features are also deleted as:\n",
    "    \n",
    "    1. OP_CARRIER_FL_NUM, like TAIL_NUM, does not provide meaningful information. Furthermore, being a feature that expresses a highly specific property, there would be a risk of making the classifier learn the values ​​by heart without it being able to exploit its generalization skills.\n",
    "    \n",
    "    2. ARR_TIME and DEP_TIME, are replaced by information about the arrival and departure time blocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.drop(columns=[\"TAIL_NUM\",\"OP_CARRIER_FL_NUM\",\"ARR_TIME\",\"DEP_TIME\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_data.loc[delays_data[\"DISTANCE\"]==31].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the existence of a 31-mile route from WRG airport (iata https://en.wikipedia.org/wiki/Wrangell_Airport) to PSG airport (iata https://en.wikipedia.org/wiki/ Petersburg_James_A._Johnson_Airport).\n",
    "\n",
    "Therefore, contrary to what one might think, 31 miles does not represent an outlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(delays_data, test_size=0.03, random_state=42)\n",
    "train_set.reset_index(inplace=True)\n",
    "test_set.reset_index(inplace=True)\n",
    "train_set.drop(columns=\"index\",inplace = True)\n",
    "test_set.drop(columns=\"index\",inplace = True)\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new variable is defined, **train_set_graph**, which represents a copy of the train set generated in the previous step. The definition of this variable allows us to generate graphs in the **DATA_UNDERSTANDING** section, making use of attributes that will be eliminated from the **train_set** used in the definition of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_graph = train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is train set è balanced? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set[train_set[\"ARR_DEL15\"]==0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set[train_set[\"ARR_DEL15\"]==1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eliminate the target attribute on the **train_set** and on the **test_set**, not before having saved the information relating to the aforementioned attribute in appropriate variables, such as **train_set_labels** and **test_set_labels**. This information will later be used in the model training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_labels = np.squeeze(train_set[\"ARR_DEL15\"])\n",
    "test_set_labels = np.squeeze(test_set[\"ARR_DEL15\"])\n",
    "\n",
    "for set_ in (train_set, test_set):\n",
    "    set_.drop(columns = [\"ARR_DEL15\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dataset is unbalanced as the distribution of the class attribute in the training set is not uniform. To confirm what has been said, the results obtained from the two previous prints can be observed, which express a clear prevalence of the \"flight not delayed\" class attribute encoded by the value 0, compared to the value 1, or \"flight delayed\".\n",
    "Note that if the dataset was not adequately balanced, following training, classifiers would tend to prefer the majority class as they are unable to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance\n",
    "\n",
    "In order to balance the train set, we made use of the RandomUnderSampler class specifying the number of records for each value taken by the class attribute. This method refers to the UnderSampling technique which envisages reducing the number of records pertaining to the majority class.\n",
    "Note that different methodologies could have been applied, such as oversampling (opposite approach to undersampling), in order to resolve the imbalance, but given the large size of the dataset it was decided to reduce the number of records of the class negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "rus = RandomUnderSampler(    \n",
    "      sampling_strategy={\n",
    "          0: 15000,\n",
    "          1: 15000\n",
    "      }, random_state = 42)\n",
    "    \n",
    "train_set_res, train_set_labels_res = rus.fit_resample(train_set, train_set_labels)\n",
    "\n",
    "train_set_res.shape,train_set_labels_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_res.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns update:\n",
    "\n",
    "Two new categorical attributes are defined, respectively **DAY_TYPE** and **TYPE_OF_FLIGHT**\n",
    "\n",
    "**Note that the operations carried out have the objective of considerably reducing the number of unique values ​​characterizing the features**\n",
    "\n",
    "Considering the high correlation between the **DAY_OF_WEEK** and **DAY_OF_MONTH** attribute, and that the **DAY_OF_MONTH** attribute alone gives clear information about the flight date, it was decided to delete **DAY_OF_WEEK** , in favor of adding a new categorical attribute that indicates whether the day of the week is a working day or relative to the weekend (working day, dd <5; week_end, gg> = 5). This choice also depends on the **OneHotEncoder**, in fact by eliminating the **DAY_OF_WEEK** attribute, the number of columns produced following the application of the aforementioned transformation function is reduced.\n",
    "\n",
    "Due to the high specificity of the distances characterizing the flights, it was decided to transform this feature into an ordinal categorical attribute that classifies the type of flight based on the distance traveled. Specifically, the flight can be of short, medium or long type.\n",
    "\n",
    "Furthermore, always considering the output of the **OneHotEncoder**, in order to avoid incurring the phenomenon of the curse of dimensionality, the information on the airports of origin and destination are replaced with the names of the countries they belong to.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_state(x):\n",
    "    row = airports_data[airports_data.IATA == x]['STATE']\n",
    "    state = row.iloc[0]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def update_columns(dataset):   \n",
    "    \n",
    "    dataset[\"DAY_TYPE\"] = np.where((dataset[\"DAY_OF_WEEK\"].astype(int)<5),\"Working day\",\"Week end day\")\n",
    "\n",
    "    dataset[\"TYPE_OF_FLIGHT\"] = np.where((dataset[\"DISTANCE\"].astype(float)<700),\"Short flight\",\"Medium flight\")\n",
    "    dataset[\"TYPE_OF_FLIGHT\"] = np.where(((dataset[\"TYPE_OF_FLIGHT\"]!=\"Short flight\")  & (dataset[\"DISTANCE\"].astype(float)>3000)),\"Long flight\",dataset[\"TYPE_OF_FLIGHT\"])\n",
    "    \n",
    "    dataset[\"ORIGIN\"] = dataset[\"ORIGIN\"].apply(lambda x : search_state(x))\n",
    "    dataset[\"DEST\"] = dataset[\"DEST\"].apply(lambda x : search_state(x))\n",
    "\n",
    "    dataset.drop(columns=[\"DAY_OF_WEEK\",\"DISTANCE\"],inplace = True)\n",
    "\n",
    "    print(dataset.columns)\n",
    "    \n",
    "    pd.DataFrame(dataset).info()\n",
    "    \n",
    "    print(\"------------end update_columns\\n\")\n",
    "\n",
    "    return pd.DataFrame(dataset)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"upd_col\",FunctionTransformer(update_columns,validate=False)),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore',)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_attribs = [\"ARR_TIME_BLK\",\"DEP_TIME_BLK\",\"OP_CARRIER\",\"DUR\",\"DAY_OF_MONTH\"]\n",
    "pipe_cat = [\"DAY_OF_WEEK\",\"ORIGIN\",\"DEST\",\"DISTANCE\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"pipe\", pipeline, pipe_cat),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore',), cat_attribs),\n",
    "    ])\n",
    "\n",
    "train_set_prepared = full_pipeline.fit_transform(train_set_res)\n",
    "test_set_prepared = full_pipeline.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_prepared,test_set_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import IncrementalPCA\n",
    "#s_IPCA = IncrementalPCA(n_components=200)\n",
    "#train_set_prepared = s_IPCA.fit_transform(train_set_prepared)\n",
    "#test_set_prepared = s_IPCA.fit_transform(test_set_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set_prepared.shape,test_set_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "\n",
    "Done one **train set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bar plot of aggregate_op per op_carrier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "#Some settings \n",
    "df_carieer = pd.DataFrame(train_set_graph['OP_CARRIER'].value_counts().reset_index().values, columns=[\"OP_CARRIER\", \"AGGREGATE_OP\"]).merge(airlines_data,left_on=\"OP_CARRIER\",right_on=\"IATA_CODE\")\n",
    "df_carieer = df_carieer.drop(columns=\"OP_CARRIER\")\n",
    "df_carieer[\"AGGREGATE_OP\"] = pd.to_numeric(df_carieer[\"AGGREGATE_OP\"])\n",
    "df_carieer.info()\n",
    "\n",
    "#sns.scatterplot(x='IATA_CODE', y='AGGREGATE_OP',data=df_carieer,palette=sns.blend_palette(['blue','red'],18),hue=\"AIRLINE\")\n",
    "\n",
    "px.scatter(df_carieer,x='AIRLINE', y='AGGREGATE_OP',size=\"AGGREGATE_OP\",color=\"AGGREGATE_OP\", color_continuous_scale=\"Dense\",title=\"Aggregate_op per op_carrier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.DataFrame(train_set_graph['ORIGIN'].value_counts().reset_index().values, columns=[\"ORIGIN\", \"ORIGIN_AGGREGATE\"])\n",
    "df_dest = pd.DataFrame(train_set_graph['DEST'].value_counts().reset_index().values, columns=[\"DEST\", \"DEST_AGGREGATE\"])\n",
    "#print(df_origin)\n",
    "#print(df_dest)\n",
    "df_tot = df_origin.merge(df_dest,left_on=\"ORIGIN\",right_on=\"DEST\")\n",
    "df_tot['TOT_ARR_DEP'] = df_tot.loc[:,[\"ORIGIN_AGGREGATE\",\"DEST_AGGREGATE\"]].sum(axis=1)\n",
    "df_tot = df_tot.merge(airports_data,left_on=\"ORIGIN\",right_on=\"IATA\")\n",
    "df_tot.drop(columns=[\"DEST\",\"ORIGIN\"],inplace=True)\n",
    "print(df_tot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So note that, df_origin contains one more row that isn't in df_dest. Without loss of generality we can consider flights with origin and destination airporst known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 Airports Arrival & Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(data=df3[0:20],x=\"ORIGIN\",y=\"TOT_ARR_DEP\",color='red').set_title(\"Top 20 Airports Arrival & Destination\")\n",
    "px.line(df_tot.loc[:50],x=\"IATA\",y=\"TOT_ARR_DEP\",markers=True,title=\"Top 50 airports Arrival & Destination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 Airports Arrival**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_origin[:50],x=\"ORIGIN\",y=\"ORIGIN_AGGREGATE\",markers=True,title=\"Top 50 Airports Arrival\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 Airports Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_dest[:50],x=\"DEST\",y=\"DEST_AGGREGATE\",markers=True,title=\"Top 50 Airports Destination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flights per date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_graph['DATE'] = pd.to_datetime('2020-01-'+train_set_graph['DAY_OF_MONTH'].astype(object).apply(str))\n",
    "train_set_graph['DAY_NAME'] = train_set_graph['DATE'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = pd.DataFrame(train_set_graph.loc[:,[\"DATE\",\"DAY_NAME\"]].value_counts().reset_index().values,columns=[\"DATE\",\"DAY_NAME\",\"AGGREGATE_DATE\"])\n",
    "df_date = df_date.sort_values(by=\"DATE\")\n",
    "df_date[\"AGGREGATE_DATE\"] = pd.to_numeric(df_date[\"AGGREGATE_DATE\"])\n",
    "print(df_date)\n",
    "#sns.lineplot(data=df_date,x=\"DATE\",y=\"AGGREGATE_DATE\")\n",
    "px.scatter(df_date,x=\"DATE\",y=\"AGGREGATE_DATE\",color=\"AGGREGATE_DATE\",size=\"AGGREGATE_DATE\",color_continuous_scale=\"Dense\",title=\"Flights per date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Departures for blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep_time_blk = pd.DataFrame(train_set_graph[\"DEP_TIME_BLK\"].value_counts().reset_index().values,columns=[\"BLOCK\",\"AGGREGATE\"])\n",
    "df_dep_time_blk[\"AGGREGATE\"] = pd.to_numeric(df_dep_time_blk[\"AGGREGATE\"])\n",
    "px.bar(df_dep_time_blk,x=\"BLOCK\",y=\"AGGREGATE\",color=\"AGGREGATE\",color_continuous_scale=\"Dense\",title=\"Departures for blocks\")\n",
    "#sns.barplot(data=df_dep_time_blk,x=\"BLOCK\",y=\"AGGREGATE\",palette=sns.blend_palette(['blue','red'],18)).set_title(\"Departures for block\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arrivals for blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr_time_blk = pd.DataFrame(train_set_graph[\"ARR_TIME_BLK\"].value_counts().reset_index().values,columns=[\"BLOCK\",\"AGGREGATE\"])\n",
    "df_arr_time_blk[\"AGGREGATE\"] = pd.to_numeric(df_arr_time_blk[\"AGGREGATE\"])\n",
    "px.bar(df_arr_time_blk,x=\"BLOCK\",y=\"AGGREGATE\",color=\"AGGREGATE\",color_continuous_scale=\"Dense\",title=\"Arrivals for blocks\")\n",
    "#sns.barplot(data=df_dep_time_blk,x=\"BLOCK\",y=\"AGGREGATE\",palette=sns.blend_palette(['blue','red'],18)).set_title(\"Departures for block\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flights per days in January**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = pd.DataFrame(df_date.loc[:,[\"DAY_NAME\",\"AGGREGATE_DATE\"]])\n",
    "df_name = pd.DataFrame((df_name.groupby(\"DAY_NAME\",as_index=False)[\"AGGREGATE_DATE\"]).sum()).sort_values(\"AGGREGATE_DATE\")\n",
    "print(df_name)\n",
    "px.scatter(df_name,x=\"DAY_NAME\",y=\"AGGREGATE_DATE\",color=\"AGGREGATE_DATE\",size=\"AGGREGATE_DATE\",color_continuous_scale=\"Dense\",title=\"Flights per date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airports map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_geo(\n",
    "    data_frame= df_tot, \n",
    "    lat= df_tot[\"LATITUDE\"], \n",
    "    lon=df_tot[\"LONGITUDE\"],\n",
    "    # loc\n",
    "    color = df_tot[\"TOT_ARR_DEP\"],\n",
    "    hover_name=\"AIRPORT\",\n",
    "    hover_data=[\n",
    "        \"CITY\",\n",
    "        \"STATE\"\n",
    "    ],\n",
    "    scope=\"north america\",\n",
    "    title=\"US airports\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of flights**\n",
    "\n",
    "A graph is defined about the types of flights present in the dataset\n",
    "\n",
    "        1. - <= 700 miles == Short flight\n",
    "\n",
    "        2. -> = 700 miles and <= 3000 miles == Medium flight\n",
    "\n",
    "        3. -> = 3000 miles to inf == Long flight\n",
    "\n",
    "For the choice of the correct range of values, the size of the American continent and the current classifications of airlines operating in the USA were taken into account, as reported on https://en.wikipedia.org/wiki/Flight_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_graph[\"TYPE_OF_FLIGHT\"] = np.where((train_set_graph[\"DISTANCE\"].astype(float)<700),\"Short flight\",\"Medium flight\")\n",
    "train_set_graph[\"TYPE_OF_FLIGHT\"] = np.where(((train_set_graph[\"TYPE_OF_FLIGHT\"]!=\"Short flight\")  & (train_set_graph[\"DISTANCE\"].astype(float)>3000)),\"Long flight\",train_set_graph[\"TYPE_OF_FLIGHT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_of_flight = pd.DataFrame(train_set_graph['TYPE_OF_FLIGHT'].value_counts().reset_index().values, columns=[\"TYPE_OF_FLIGHT\", \"TYPE_OF_FLIGHT_AGGREGATE\"])\n",
    "print(df_type_of_flight)\n",
    "px.bar(df_type_of_flight,x=\"TYPE_OF_FLIGHT\",y=\"TYPE_OF_FLIGHT_AGGREGATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "\n",
    "dt = datetime.datetime.now()\n",
    "str_date_time = dt.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "#result_filename = \"results_\"+str_date_time\n",
    "result_filename = 'results_2'\n",
    "print(result_filename)\n",
    "\n",
    "dataResults= pd.DataFrame(columns=[\n",
    "  'Classifier','Type of test',\n",
    "  'Accuracy', 'Precision', 'Recall', 'F1', 'Balanced accuracy', 'Weighted Precision', 'Weighted Recall', 'Weighted F1'\n",
    "  ])\n",
    "  \n",
    "print(dataResults.head())\n",
    "\n",
    "def upload(classifier,type_of_test,\n",
    "  accuracy, precision,recall,f1,\n",
    "  balanced_accuracy, weighted_precision, weighted_recall,weighted_f1):\n",
    "\n",
    "  dataResults = None\n",
    "\n",
    "  try:\n",
    "    dataResults = pd.read_csv(\"./Result/\"+result_filename+\".csv\")\n",
    "  except FileNotFoundError:\n",
    "    open(\"./Result/\"+result_filename+\".csv\",'w')\n",
    "\n",
    "  try:\n",
    "    dataResults = pd.read_csv(\"./Result/\"+result_filename+\".csv\")\n",
    "  except pd.errors.EmptyDataError:\n",
    "      print('CSV empty')\n",
    "      dataResults = pd.DataFrame()\n",
    "\n",
    "  dict = {\n",
    "    \"Classifier\" : classifier,\n",
    "    \"Type of test\" : type_of_test,\n",
    "    \"Accuracy\" : accuracy,\n",
    "    \"Precision\" : precision,\n",
    "    \"Recall\" : recall,\n",
    "    \"F1\" : f1,\n",
    "    \"Balanced Accuracy\" : balanced_accuracy,\n",
    "    \"Weighted Precision\":weighted_precision,\n",
    "    \"Weighted Recall\":weighted_recall,\n",
    "    \"Weighted F1\":weighted_f1\n",
    "    }\n",
    "  dataResults=dataResults.append(dict, ignore_index=True)\n",
    "  dataResults.to_csv(\"./Result/\"+result_filename+\".csv\",index=False)\n",
    "  return dataResults\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that following the predictions, are saved the measure below:\n",
    "    \n",
    "    1. Accuracy,\n",
    "    2. Precision,\n",
    "    3. Recall,\n",
    "    4. F1,\n",
    "\n",
    "balanced, and weighted measure such as:\n",
    "\n",
    "    1. Balanced Accuracy,\n",
    "    2. Weighted Precision,\n",
    "    3. Weighted Recall,\n",
    "    4. Weighted F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** metric evaluates the fraction of samples correctly labeled on the total of records present in the dataset. This metric is not adequate if the classes contain a highly different number of records, or if the dataset is unbalanced. Considering that balancing operations only make sense if applied to the train set, it was necessary to introduce another metric that takes into account the unbalancing of the test set in the evaluation of accuracy. For this reason the **balanced_accuracy_score** was introduced.\n",
    "\n",
    "In addition to the accuracy metric, and its version for unbalanced datasets, metrics such as **Precision**, **Recall** and **F1** were introduced, respectively used for:\n",
    "\n",
    "    1. Evaluate the relationship between records correctly classified as positive and records classified as positive.\n",
    "    2. Evaluate the relationship between positive records and correctly classified ones.\n",
    "    3. Evaluate the harmonic mean between precision and recall.\n",
    "\n",
    "Equivalent to the speech made previously for accuracy, it was decided to introduce appropriate modifications on the Precision, Recall and F1 metrics in order to adapt them to properly evaluate scores on unbalanced datasets (test set).\n",
    "This readjustment required the inclusion of an **average** parameter in the definition of the class of the metric taken into consideration. This parameter takes the value: **weighted**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the imbalance of the test set it was decided to train the classifiers with respect to the F1 metric (F-measure). Specifically, this choice is due to the desire to take into good consideration both the Precision and the Recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def get_metrics(pred,labels):\n",
    "    ret = {\n",
    "        \"accuracy\" : accuracy_score(pred,labels),\n",
    "        \"precision\" : precision_score(pred,labels),\n",
    "        \"recall\" : recall_score(pred,labels),\n",
    "        \"f1\" : f1_score(pred,labels),\n",
    "        \"balanced_accuracy\" : balanced_accuracy_score(pred,labels),\n",
    "        \"balanced_precision\" : precision_score(pred,labels,average = 'weighted'),\n",
    "        \"balanced_recall\" : recall_score(pred,labels,average = 'weighted'),\n",
    "        \"balanced_f1\" : f1_score(pred,labels,average = 'weighted')\n",
    "    }\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "filename = './Model/rnd_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    rnd_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(rnd_clf)\n",
    "except FileNotFoundError:\n",
    "    rnd_clf = RandomForestClassifier()\n",
    "    rnd_clf.fit(train_set_prepared,train_set_labels_res)\n",
    "    # save the model to disk\n",
    "    pickle.dump(rnd_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnd = rnd_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_rnd,train_set_labels_res)\n",
    "#print(classification_report(y_pred_rnd,train_set_labels_res))\n",
    "upload('Random forest','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1']\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnd = rnd_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_rnd,test_set_labels)\n",
    "print(classification_report(y_pred_rnd,test_set_labels))\n",
    "upload('Random forest','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning \n",
    "The goal is to improve the performance of the classifier by tuning the parameters\n",
    "\n",
    "##### Grid Search\n",
    "It was decided to use the GridSearchCV made available by Scikit-Learn's in order to find the best configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "filename = './Model/best_rnd_clf.sav'\n",
    "\n",
    "try:\n",
    "    best_rnd_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(best_rnd_clf)\n",
    "except FileNotFoundError:\n",
    "  # hyperparameters\n",
    "  param_grid = [\n",
    "    {'n_estimators': [10, 20, 35], 'max_features': [10, 15, 20]},\n",
    "  ]\n",
    "  # define grid search\n",
    "  best_rnd_clf = RandomForestClassifier()\n",
    "  grid_search_rnd = GridSearchCV(best_rnd_clf, param_grid, cv=5,scoring='f1', return_train_score=True, n_jobs=4)\n",
    "  # fit\n",
    "  grid_search_rnd.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "  print(\"Best Param:\", grid_search_rnd.best_params_)\n",
    "  best_rnd_clf = grid_search_rnd.best_estimator_\n",
    "\n",
    "  # save the model to disk\n",
    "  pickle.dump(best_rnd_clf, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnd= best_rnd_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_rnd,train_set_labels_res)\n",
    "print(classification_report(y_pred_rnd,train_set_labels_res))\n",
    "upload('Random forest','Train Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnd= best_rnd_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_rnd,test_set_labels)\n",
    "print(classification_report(y_pred_rnd,test_set_labels))\n",
    "upload('Random forest','Test Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "filename = './Model/sgd_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    sgd_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(sgd_clf)\n",
    "except FileNotFoundError:\n",
    "    sgd_clf = SGDClassifier()\n",
    "    sgd_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "    # save the model to disk\n",
    "    pickle.dump(sgd_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = sgd_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_sgd,train_set_labels_res)\n",
    "print(classification_report(y_pred_sgd,train_set_labels_res))\n",
    "upload('SGD','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = sgd_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_sgd,test_set_labels)\n",
    "print(classification_report(y_pred_sgd,test_set_labels))\n",
    "upload('SGD','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "filename = './Model/best_sgd_clf.sav'\n",
    "\n",
    "try:\n",
    "    best_sgd_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(best_sgd_clf)\n",
    "except FileNotFoundError:\n",
    "    # hyperparameters\n",
    "    params = {\n",
    "        'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'penalty': ['l2','elasticnet', 'l1']\n",
    "    }\n",
    "    # define grid search\n",
    "    best_sgd_clf = SGDClassifier()\n",
    "    grid_search_sgd = GridSearchCV(best_sgd_clf, param_grid = params, cv=5,scoring='f1',n_jobs=4)\n",
    "    # fit\n",
    "    grid_search_sgd.fit(train_set_prepared, train_set_labels_res)\n",
    "    print(\"Best Param:\", grid_search_sgd.best_params_)\n",
    "    best_sgd_clf = grid_search_sgd.best_estimator_\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(best_sgd_clf, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd= best_sgd_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_sgd,train_set_labels_res)\n",
    "print(classification_report(y_pred_sgd,train_set_labels_res))\n",
    "upload('SGD','Train Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd= best_sgd_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_sgd,test_set_labels)\n",
    "print(classification_report(y_pred_sgd,test_set_labels))\n",
    "upload('SGD','Test Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "filename = './Model/l_svc_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    l_svc_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(l_svc_clf)\n",
    "except FileNotFoundError:\n",
    "    l_svc_clf = LinearSVC()\n",
    "    l_svc_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "    # save the model to disk\n",
    "    pickle.dump(l_svc_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l_svc= l_svc_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_l_svc,train_set_labels_res)\n",
    "print(classification_report(y_pred_l_svc,train_set_labels_res))\n",
    "upload('Linear SVC','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l_svc= l_svc_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_l_svc,test_set_labels)\n",
    "print(classification_report(y_pred_l_svc,test_set_labels))\n",
    "upload('Linear SVC','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "filename = './Model/best_l_svc_clf.sav'\n",
    "\n",
    "try:\n",
    "    best_l_svc_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(best_l_svc_clf)\n",
    "except FileNotFoundError:\n",
    "    # hyperparameters\n",
    "    params = {\n",
    "        'loss': ['squared_hinge'],\n",
    "    }\n",
    "    # define grid search\n",
    "    best_l_svc_clf = LinearSVC(max_iter = 100,dual=False)\n",
    "    grid_search_l_svc = GridSearchCV(best_l_svc_clf, param_grid = params, cv=5,scoring='f1',n_jobs=4)\n",
    "    # fit\n",
    "    grid_search_l_svc.fit(train_set_prepared, train_set_labels_res)\n",
    "    print(\"Best Param:\", grid_search_l_svc.best_params_)\n",
    "    best_l_svc_clf = grid_search_l_svc.best_estimator_\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(best_l_svc_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l_svc= best_l_svc_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_l_svc,train_set_labels_res)\n",
    "print(classification_report(y_pred_l_svc,train_set_labels_res))\n",
    "upload('Linear SVC','Train Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l_svc= best_l_svc_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_l_svc,test_set_labels)\n",
    "print(classification_report(y_pred_l_svc,test_set_labels))\n",
    "upload('Linear SVC','Test Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic-Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = './Model/logreg_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    logreg_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(logreg_clf)\n",
    "except FileNotFoundError:\n",
    "    logreg_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    logreg_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(logreg_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = logreg_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_logreg,train_set_labels_res)\n",
    "print(classification_report(y_pred_logreg,train_set_labels_res))\n",
    "upload('Logistic Regressor','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],\n",
    "report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = logreg_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_logreg,test_set_labels)\n",
    "print(classification_report(y_pred_logreg,test_set_labels))\n",
    "upload('Logistic Regressor','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],\n",
    "report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = './Model/best_logreg_clf.sav'\n",
    "\n",
    "try:\n",
    "    best_logreg_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(best_logreg_clf)\n",
    "except FileNotFoundError:\n",
    "    # hyperparameters\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    penalty = ['l2']\n",
    "    c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    best_logreg_clf = LogisticRegression()\n",
    "    grid_search_logreg = GridSearchCV(best_logreg_clf, param_grid=grid, cv=10, scoring='f1', return_train_score=True, n_jobs=4)\n",
    "    # fit \n",
    "    grid_search_logreg.fit(train_set_prepared, train_set_labels_res)\n",
    "    print(\"Best Param:\", grid_search_logreg.best_params_)\n",
    "    best_logreg_clf = grid_search_logreg.best_estimator_\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(best_logreg_clf, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg= best_logreg_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_logreg,train_set_labels_res)\n",
    "print(classification_report(y_pred_logreg,train_set_labels_res))\n",
    "upload('Logistic Regressor','Train Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg= best_logreg_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_logreg,test_set_labels)\n",
    "print(classification_report(y_pred_logreg,test_set_labels))\n",
    "upload('Logistic Regressor','Test Grid',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "filename = './Model/knn_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    knn_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(knn_clf)\n",
    "except FileNotFoundError:\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    knn_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(knn_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_KNN= knn_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_KNN,train_set_labels_res)\n",
    "print(classification_report(y_pred_KNN,train_set_labels_res))\n",
    "upload('KNN','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_KNN= knn_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_KNN,test_set_labels)\n",
    "print(classification_report(y_pred_KNN,test_set_labels))\n",
    "upload('KNN','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative:\n",
    "\n",
    "The radius-based approach to selecting neighbors is more appropriate for sparse data, preventing records that are distant from contributing to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "filename = './Model/r_knn_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    r_knn_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(r_knn_clf)\n",
    "except FileNotFoundError:\n",
    "    r_knn_clf = RadiusNeighborsClassifier(radius=1.8,outlier_label='most_frequent')\n",
    "    r_knn_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(r_knn_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_r_KNN= r_knn_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_r_KNN,train_set_labels_res)\n",
    "print(classification_report(y_pred_r_KNN,train_set_labels_res))\n",
    "upload('Radius KNN','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_r_KNN= r_knn_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_r_KNN,test_set_labels)\n",
    "print(classification_report(y_pred_r_KNN,test_set_labels))\n",
    "upload('Radius KNN','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import *\n",
    "filename = './Model/mpl_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    mpl_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(mpl_clf)\n",
    "except FileNotFoundError:\n",
    "    mpl_clf = MLPClassifier(max_iter=500)\n",
    "    mpl_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(mpl_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP = mpl_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_MLP,train_set_labels_res)\n",
    "print(classification_report(y_pred_MLP,train_set_labels_res))\n",
    "upload('MLP','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP = mpl_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_MLP,test_set_labels)\n",
    "print(classification_report(y_pred_MLP,test_set_labels))\n",
    "upload('MLP','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Decision-Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = './Model/dc_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    dc_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(dc_clf)\n",
    "except FileNotFoundError:\n",
    "    dc_clf = DecisionTreeClassifier()\n",
    "    dc_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(dc_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DC = dc_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_DC,train_set_labels_res)\n",
    "print(classification_report(y_pred_DC,train_set_labels_res))\n",
    "upload('Decision Tree','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DC = dc_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_DC,test_set_labels)\n",
    "print(classification_report(y_pred_DC,test_set_labels))\n",
    "upload('Decision Tree','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ada-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "filename = './Model/adaBoost_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    adaBoost_clf = pickle.load(open(filename, 'rb'))\n",
    "    print(adaBoost_clf)\n",
    "except FileNotFoundError:\n",
    "    adaBoost_clf = AdaBoostClassifier()\n",
    "    adaBoost_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(adaBoost_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ADABOOST = adaBoost_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_ADABOOST,train_set_labels_res)\n",
    "print(classification_report(y_pred_ADABOOST,train_set_labels_res))\n",
    "upload('Ada Boost','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ADABOOST = adaBoost_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_ADABOOST,test_set_labels)\n",
    "print(classification_report(y_pred_ADABOOST,test_set_labels))\n",
    "upload('Ada Boost','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation of classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataResults = pd.read_csv(\"./Result/\"+result_filename+\".csv\")\n",
    "dataResults = pd.read_csv(\"./Result/results_original.csv\")\n",
    "\n",
    "dataResults.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "filename = './Model/voting_clf.sav'\n",
    "\n",
    "# load the model from disk\n",
    "try:\n",
    "    voting_clf  = pickle.load(open(filename, 'rb'))\n",
    "    print(voting_clf)\n",
    "except FileNotFoundError:\n",
    "    voting_clf = VotingClassifier(estimators=[('log_reg', best_logreg_clf), ('mlp', mpl_clf), ('l_svc', l_svc_clf)], voting='hard')\n",
    "    voting_clf.fit(train_set_prepared, train_set_labels_res)\n",
    "\n",
    "    # save the model to disk\n",
    "    pickle.dump(voting_clf , open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_VOTING = voting_clf.predict(train_set_prepared)\n",
    "report = get_metrics(y_pred_VOTING,train_set_labels_res)\n",
    "print(classification_report(y_pred_VOTING,train_set_labels_res))\n",
    "upload('Voting','Train',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance check on the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_VOTING = voting_clf.predict(test_set_prepared)\n",
    "report = get_metrics(y_pred_VOTING,test_set_labels)\n",
    "print(classification_report(y_pred_VOTING,test_set_labels))\n",
    "upload('Voting','Test',\n",
    "report['accuracy'],report['precision'],report['recall'],report['f1'],\n",
    "report['balanced_accuracy'],report['balanced_precision'],report['balanced_recall'],report['balanced_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the initial section, the aim of this analysis was to define whether or not an airplane flight was delayed, where a delayed flight means a flight that arrives at its destination with at least 15 minutes delay. Note that instead of considering one of **DEP_DEL15** or **ARR_DEL15** as the target attribute, a new categorical attribute could have been defined, for example **delayed_flight** as a combination in **or** or **and** of the two previous attributes.\n",
    "\n",
    "Considering the high number of features, about 260, two different dimensionality reduction techniques were applied in order to verify the existence of any subset of features on which to train the classifiers.\n",
    "Specifically, in consideration of the encoding output (sparse matrix), the Truncated-SVD ​​and Incremental-PCA classes were used as the methods of these classes allow to operate efficiently on inputs of this type.\n",
    "Both classes perform linear reductions, with the aim of identifying the main features present in a data set.\n",
    "\n",
    "Following the application of the methods made available by the aforementioned classes, two sets of reduced-size features have been identified, each characterized by 200 features. Once the identification of the main components was completed, the classifiers were trained on these features. The scores obtained following the prediction activities are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataResults_IPCA = pd.read_csv(\"./Result/results_IPCA_200.csv\")\n",
    "dataResults_IPCA.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataResults_TSVD = pd.read_csv(\"./Result/results_TSVD_200.csv\")\n",
    "dataResults_TSVD.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataResults_original = pd.read_csv(\"./Result/results_original.csv\")\n",
    "dataResults_original.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As highlighted by the previous tables, the scores obtained on a reduced version of the number of components produced worse results than the scores obtained by classifiers trained on the original number of components. To understand the reason for these results, consider the following example:\n",
    "\n",
    "Suppose a simple case with 3 independent variables x1,x2,x3\n",
    "and the output y and suppose now that x3=y\n",
    "\n",
    "and so you should be able to get a 0 error model.\n",
    "\n",
    "Suppose now that in the training set the variation of y\n",
    "is very small and so also the variation of x3.\n",
    "\n",
    "Now if you run PCA and you decide to select only 2 variables you will obtain a combination of x1\n",
    "and x2. So the information of x3 that was the only variable able to explain y is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the excessive number of features depended on the application of the oneHotEncoder. In order to reduce the number of features generated by the encoding activity, such encoding could have been replaced with a process that transforms catarogical attributes into numeric based on the frequency of each attribute. By doing so rare attributes, they would be encoded with the same frequency thus reducing the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the imbalance of the test set, different tests were performed in order to identify the best metrics in order to correctly evaluate the results. If this fundamental phase had not been carried out, valid scores would have been produced for the majority class and not very significant for the minority class. Specifically, good precision values ​​would have been obtained at the expense of low Recall values ​​as the algorithms would have had difficulty capturing the relationships between the attributes and the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for some classifiers a Fine Tuning phase was carried out, applying the grid search. This procedure has not been applied to all classifiers as it is computationally burdensome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers:\n",
    "\n",
    "    1. Random Forest\n",
    "    2. SGD\n",
    "    3. Linear SVC\n",
    "    4. Logistic Regressor\n",
    "    5. KNN\n",
    "    6. Radius KNN\n",
    "    7. MLP\n",
    "    8. Decision Tree\n",
    "    9. AdaBoost\n",
    "    10. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataResults_original[dataResults_original['Type of test']=='Train']\n",
    "a.sort_values(by=['Accuracy'], inplace=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "fig = sns.barplot(x=a['Classifier'], y=a['Balanced Accuracy'], palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Performance on balanced accuracy', size=20)\n",
    "plt.show()\n",
    "figure = fig.get_figure()\n",
    "#save_figure(figure,\"Performance accuracy classifiers TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dataResults_original[dataResults_original['Type of test']=='Test']\n",
    "b.sort_values(by=['Balanced Accuracy'], inplace=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "fig = sns.barplot(x=b['Classifier'], y=b['Balanced Accuracy'], palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Performance on balanced accuracy (test set)', size=20)\n",
    "plt.show()\n",
    "figure = fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dataResults_original[dataResults_original['Type of test']=='Test']\n",
    "c.sort_values(by=['Weighted F1'], inplace=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "fig = sns.barplot(x=c['Classifier'], y=c['Weighted F1'], palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Performance on Weighted F1 (test set)', size=20)\n",
    "plt.show()\n",
    "figure = fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataResults_original[dataResults_original['Type of test']=='Test']\n",
    "d.sort_values(by=['Weighted Precision'], inplace=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "fig = sns.barplot(x=d['Classifier'], y=d['Weighted Precision'], palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Weighted Precision')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Performance on Weighted Precision (test set)', size=20)\n",
    "plt.show()\n",
    "figure = fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = dataResults_original[dataResults_original['Type of test']=='Test']\n",
    "e.sort_values(by=['Weighted Recall'], inplace=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "fig = sns.barplot(x=e['Classifier'], y=e['Weighted Recall'], palette='deep')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Weighted Recall')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title('Performance on Weighted Recall (test set)', size=20)\n",
    "plt.show()\n",
    "figure = fig.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the application of the K fold cross validation technique could allow the identification of the best possible division of the dataset into train and test set at the expense of computation complexity as k different classifiers would be generated.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1143792cea51aa2325838ddcdcdd187f8f9ab85d068df8cbfdff039106e982a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
